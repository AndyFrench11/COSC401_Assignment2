{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC401 Assignment 2\n",
    "## Andrew French \n",
    "### ID: 11147452"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is all of the libraries and modules needed for the entire program being imported. \n",
    "Make sure to run this step before running any of the feature code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is all of the variables set that are global to the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19680801)\n",
    "plt.rcParams['figure.figsize'] = (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Gradient-based Learning with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you should perform a regression task with tensor operations on\n",
    "synthetic data generated by yourself.\n",
    "You must first come up with a function that maps R\n",
    "m to R\n",
    "n with both m\n",
    "and n parametrically defined at the beginning of your program and set to values\n",
    "greater than 3. Provide the equation of the function(s). Add some noise to the\n",
    "output to make the problem more interesting. However, do not add too much\n",
    "noise so that the underlying functions are still recognisable. Also be mindful of\n",
    "the expected value of the noise function.\n",
    "Pass the generated data to your own implementation of linear regression\n",
    "algorithm to see if it produces, more or less (depending on how much noise was\n",
    "added), the expected model.\n",
    "Further remarks and requirements:\n",
    "• You are not allowed to use any regression libraries. You must use only\n",
    "tensor operations and automatic gradient calculations to write your loss\n",
    "function and to write your own optimisation procedure based on gradient\n",
    "descent. Do not use any sub-modules in PyTorch (e.g. neural networks or\n",
    "optimisation). Everything must be by tensor operations.\n",
    "• When possible, avoid using loops; instead use tensor operations to act on\n",
    "multiple values at once.\n",
    "• Avoid using matrix inverse operators (use only gradients);\n",
    "• When performing updates, be mindful of gradient tracking.\n",
    "• Make sure you generate enough data.\n",
    "2\n",
    "• Choose appropriate learning rate.\n",
    "• Choose an appropriate termination criteria for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29845572.0\n",
      "1 23038610.0\n",
      "2 20647116.0\n",
      "3 19256916.0\n",
      "4 17356730.0\n",
      "5 14354616.0\n",
      "6 10833276.0\n",
      "7 7505711.5\n",
      "8 4964562.0\n",
      "9 3234343.75\n",
      "10 2152369.0\n",
      "11 1490053.875\n",
      "12 1085075.625\n",
      "13 829644.3125\n",
      "14 661532.0\n",
      "15 544949.9375\n",
      "16 459920.5\n",
      "17 395015.21875\n",
      "18 343591.25\n",
      "19 301632.625\n",
      "20 266658.03125\n",
      "21 237063.40625\n",
      "22 211719.875\n",
      "23 189808.15625\n",
      "24 170737.75\n",
      "25 154028.5\n",
      "26 139323.875\n",
      "27 126303.7109375\n",
      "28 114776.3046875\n",
      "29 104524.90625\n",
      "30 95379.4140625\n",
      "31 87199.7109375\n",
      "32 79850.9453125\n",
      "33 73236.65625\n",
      "34 67273.3046875\n",
      "35 61880.05859375\n",
      "36 56992.11328125\n",
      "37 52554.46484375\n",
      "38 48518.3671875\n",
      "39 44839.3828125\n",
      "40 41482.31640625\n",
      "41 38415.86328125\n",
      "42 35607.84765625\n",
      "43 33033.93359375\n",
      "44 30673.009765625\n",
      "45 28505.41796875\n",
      "46 26511.365234375\n",
      "47 24675.396484375\n",
      "48 22981.337890625\n",
      "49 21416.990234375\n",
      "50 19972.62109375\n",
      "51 18637.568359375\n",
      "52 17402.33203125\n",
      "53 16257.27734375\n",
      "54 15195.330078125\n",
      "55 14210.0263671875\n",
      "56 13295.6015625\n",
      "57 12445.5390625\n",
      "58 11654.9306640625\n",
      "59 10919.2216796875\n",
      "60 10233.9208984375\n",
      "61 9596.4580078125\n",
      "62 9002.927734375\n",
      "63 8448.7451171875\n",
      "64 7931.85595703125\n",
      "65 7450.37841796875\n",
      "66 7000.3232421875\n",
      "67 6579.466796875\n",
      "68 6186.0185546875\n",
      "69 5818.07958984375\n",
      "70 5473.52490234375\n",
      "71 5151.0439453125\n",
      "72 4848.91943359375\n",
      "73 4565.857421875\n",
      "74 4300.35693359375\n",
      "75 4051.32958984375\n",
      "76 3817.9775390625\n",
      "77 3599.66015625\n",
      "78 3394.759521484375\n",
      "79 3202.29052734375\n",
      "80 3021.458251953125\n",
      "81 2851.454345703125\n",
      "82 2691.635986328125\n",
      "83 2541.5068359375\n",
      "84 2400.25390625\n",
      "85 2267.41259765625\n",
      "86 2142.400146484375\n",
      "87 2024.6302490234375\n",
      "88 1913.7366943359375\n",
      "89 1809.309814453125\n",
      "90 1710.8924560546875\n",
      "91 1618.138427734375\n",
      "92 1530.70947265625\n",
      "93 1448.251220703125\n",
      "94 1370.480712890625\n",
      "95 1297.1259765625\n",
      "96 1227.9130859375\n",
      "97 1162.5870361328125\n",
      "98 1101.0203857421875\n",
      "99 1042.874267578125\n",
      "100 987.975830078125\n",
      "101 936.1235961914062\n",
      "102 887.1240234375\n",
      "103 840.8132934570312\n",
      "104 797.0552978515625\n",
      "105 755.679931640625\n",
      "106 716.5487670898438\n",
      "107 679.5511474609375\n",
      "108 644.54736328125\n",
      "109 611.4458618164062\n",
      "110 580.1250610351562\n",
      "111 550.4796752929688\n",
      "112 522.4149169921875\n",
      "113 495.8524475097656\n",
      "114 470.7046813964844\n",
      "115 446.8923645019531\n",
      "116 424.33648681640625\n",
      "117 402.97125244140625\n",
      "118 382.7257995605469\n",
      "119 363.54840087890625\n",
      "120 345.37127685546875\n",
      "121 328.14495849609375\n",
      "122 311.8207092285156\n",
      "123 296.33587646484375\n",
      "124 281.6519775390625\n",
      "125 267.7306213378906\n",
      "126 254.52203369140625\n",
      "127 241.99659729003906\n",
      "128 230.11129760742188\n",
      "129 218.83396911621094\n",
      "130 208.1339111328125\n",
      "131 197.9793243408203\n",
      "132 188.33737182617188\n",
      "133 179.18399047851562\n",
      "134 170.492431640625\n",
      "135 162.23995971679688\n",
      "136 154.40188598632812\n",
      "137 146.95626831054688\n",
      "138 139.88775634765625\n",
      "139 133.1680450439453\n",
      "140 126.78598022460938\n",
      "141 120.72135925292969\n",
      "142 114.96170806884766\n",
      "143 109.4893798828125\n",
      "144 104.28761291503906\n",
      "145 99.33905792236328\n",
      "146 94.63587951660156\n",
      "147 90.16414642333984\n",
      "148 85.91141510009766\n",
      "149 81.86534118652344\n",
      "150 78.01754760742188\n",
      "151 74.35601043701172\n",
      "152 70.87324523925781\n",
      "153 67.55943298339844\n",
      "154 64.40542602539062\n",
      "155 61.403236389160156\n",
      "156 58.545318603515625\n",
      "157 55.826026916503906\n",
      "158 53.23638916015625\n",
      "159 50.77048873901367\n",
      "160 48.42396545410156\n",
      "161 46.188270568847656\n",
      "162 44.05976104736328\n",
      "163 42.03174591064453\n",
      "164 40.100929260253906\n",
      "165 38.26016616821289\n",
      "166 36.50868225097656\n",
      "167 34.838768005371094\n",
      "168 33.24723815917969\n",
      "169 31.730676651000977\n",
      "170 30.285585403442383\n",
      "171 28.907840728759766\n",
      "172 27.59593963623047\n",
      "173 26.344436645507812\n",
      "174 25.151081085205078\n",
      "175 24.013559341430664\n",
      "176 22.928733825683594\n",
      "177 21.89495277404785\n",
      "178 20.908536911010742\n",
      "179 19.968019485473633\n",
      "180 19.071544647216797\n",
      "181 18.215620040893555\n",
      "182 17.399309158325195\n",
      "183 16.6206111907959\n",
      "184 15.87806224822998\n",
      "185 15.168670654296875\n",
      "186 14.492475509643555\n",
      "187 13.84692096710205\n",
      "188 13.231308937072754\n",
      "189 12.643875122070312\n",
      "190 12.08264446258545\n",
      "191 11.546950340270996\n",
      "192 11.035828590393066\n",
      "193 10.547948837280273\n",
      "194 10.08188533782959\n",
      "195 9.636855125427246\n",
      "196 9.212339401245117\n",
      "197 8.806533813476562\n",
      "198 8.419482231140137\n",
      "199 8.049723625183105\n",
      "200 7.696658134460449\n",
      "201 7.359091281890869\n",
      "202 7.036870956420898\n",
      "203 6.728937149047852\n",
      "204 6.434978485107422\n",
      "205 6.154096603393555\n",
      "206 5.886038780212402\n",
      "207 5.629268169403076\n",
      "208 5.384263515472412\n",
      "209 5.1502814292907715\n",
      "210 4.926441192626953\n",
      "211 4.712808132171631\n",
      "212 4.50856876373291\n",
      "213 4.313401222229004\n",
      "214 4.1268534660339355\n",
      "215 3.9482944011688232\n",
      "216 3.777616024017334\n",
      "217 3.614675283432007\n",
      "218 3.458949089050293\n",
      "219 3.309735059738159\n",
      "220 3.1674680709838867\n",
      "221 3.031158208847046\n",
      "222 2.9009363651275635\n",
      "223 2.776297092437744\n",
      "224 2.657292366027832\n",
      "225 2.5433273315429688\n",
      "226 2.434595823287964\n",
      "227 2.330533981323242\n",
      "228 2.2307870388031006\n",
      "229 2.135464668273926\n",
      "230 2.0442981719970703\n",
      "231 1.9571176767349243\n",
      "232 1.873805046081543\n",
      "233 1.7939021587371826\n",
      "234 1.717665672302246\n",
      "235 1.6446160078048706\n",
      "236 1.5748450756072998\n",
      "237 1.5078734159469604\n",
      "238 1.4439769983291626\n",
      "239 1.3826795816421509\n",
      "240 1.3240565061569214\n",
      "241 1.2680323123931885\n",
      "242 1.214388370513916\n",
      "243 1.163122534751892\n",
      "244 1.1139683723449707\n",
      "245 1.066959023475647\n",
      "246 1.0219666957855225\n",
      "247 0.9788825511932373\n",
      "248 0.9375678300857544\n",
      "249 0.8980985879898071\n",
      "250 0.8603157997131348\n",
      "251 0.8241826891899109\n",
      "252 0.7894970178604126\n",
      "253 0.756331741809845\n",
      "254 0.7245558500289917\n",
      "255 0.6942445039749146\n",
      "256 0.6651196479797363\n",
      "257 0.6372186541557312\n",
      "258 0.6105113625526428\n",
      "259 0.5850405097007751\n",
      "260 0.5605795979499817\n",
      "261 0.5371352434158325\n",
      "262 0.5147783756256104\n",
      "263 0.49338629841804504\n",
      "264 0.47277674078941345\n",
      "265 0.4530964195728302\n",
      "266 0.43424734473228455\n",
      "267 0.416262149810791\n",
      "268 0.39890000224113464\n",
      "269 0.3823401629924774\n",
      "270 0.36647602915763855\n",
      "271 0.3513391613960266\n",
      "272 0.336760938167572\n",
      "273 0.3227865695953369\n",
      "274 0.3094308376312256\n",
      "275 0.29660362005233765\n",
      "276 0.2843320071697235\n",
      "277 0.2726117968559265\n",
      "278 0.2613256871700287\n",
      "279 0.25052767992019653\n",
      "280 0.24022437632083893\n",
      "281 0.23029345273971558\n",
      "282 0.22081981599330902\n",
      "283 0.21169334650039673\n",
      "284 0.2030085176229477\n",
      "285 0.19468888640403748\n",
      "286 0.18667149543762207\n",
      "287 0.17897289991378784\n",
      "288 0.17162342369556427\n",
      "289 0.16458797454833984\n",
      "290 0.15783444046974182\n",
      "291 0.15135514736175537\n",
      "292 0.14512105286121368\n",
      "293 0.1391575187444687\n",
      "294 0.13348737359046936\n",
      "295 0.12802362442016602\n",
      "296 0.12278488278388977\n",
      "297 0.11774253100156784\n",
      "298 0.11290344595909119\n",
      "299 0.10829304158687592\n",
      "300 0.10387791693210602\n",
      "301 0.099635049700737\n",
      "302 0.09556146711111069\n",
      "303 0.09165764600038528\n",
      "304 0.08792531490325928\n",
      "305 0.08432967215776443\n",
      "306 0.08088695257902145\n",
      "307 0.0775754451751709\n",
      "308 0.07445148378610611\n",
      "309 0.07142013311386108\n",
      "310 0.06851325929164886\n",
      "311 0.0657227486371994\n",
      "312 0.06305117160081863\n",
      "313 0.060503751039505005\n",
      "314 0.05804869905114174\n",
      "315 0.05568774789571762\n",
      "316 0.0534212626516819\n",
      "317 0.05125151574611664\n",
      "318 0.04916492477059364\n",
      "319 0.04719293490052223\n",
      "320 0.04527294635772705\n",
      "321 0.04343915730714798\n",
      "322 0.04167255386710167\n",
      "323 0.03999597579240799\n",
      "324 0.038385022431612015\n",
      "325 0.0368356890976429\n",
      "326 0.03537153825163841\n",
      "327 0.03392999619245529\n",
      "328 0.0325666181743145\n",
      "329 0.031256675720214844\n",
      "330 0.030002810060977936\n",
      "331 0.028806444257497787\n",
      "332 0.027660101652145386\n",
      "333 0.02653765305876732\n",
      "334 0.02546798437833786\n",
      "335 0.02444932609796524\n",
      "336 0.023490356281399727\n",
      "337 0.022544478997588158\n",
      "338 0.021634912118315697\n",
      "339 0.020777955651283264\n",
      "340 0.019953787326812744\n",
      "341 0.019157681614160538\n",
      "342 0.018384551629424095\n",
      "343 0.01764513924717903\n",
      "344 0.016948796808719635\n",
      "345 0.016277043148875237\n",
      "346 0.015619611367583275\n",
      "347 0.014999444596469402\n",
      "348 0.01441627275198698\n",
      "349 0.013850050047039986\n",
      "350 0.013303602114319801\n",
      "351 0.012777532450854778\n",
      "352 0.012274665758013725\n",
      "353 0.011799066327512264\n",
      "354 0.011337067000567913\n",
      "355 0.010891508311033249\n",
      "356 0.01045981515198946\n",
      "357 0.01004955917596817\n",
      "358 0.009661104530096054\n",
      "359 0.009287545457482338\n",
      "360 0.008924240246415138\n",
      "361 0.00858166441321373\n",
      "362 0.00825352780520916\n",
      "363 0.007933055981993675\n",
      "364 0.007618207484483719\n",
      "365 0.007330015301704407\n",
      "366 0.0070555186830461025\n",
      "367 0.006787170190364122\n",
      "368 0.0065290844067931175\n",
      "369 0.00628095306456089\n",
      "370 0.00604089442640543\n",
      "371 0.00581471249461174\n",
      "372 0.005592589732259512\n",
      "373 0.005386181641370058\n",
      "374 0.005186015740036964\n",
      "375 0.004990302957594395\n",
      "376 0.004806562326848507\n",
      "377 0.004627975635230541\n",
      "378 0.004452396649867296\n",
      "379 0.004290502518415451\n",
      "380 0.004136652685701847\n",
      "381 0.003978872671723366\n",
      "382 0.0038354340940713882\n",
      "383 0.003693093080073595\n",
      "384 0.0035637449473142624\n",
      "385 0.003436413360759616\n",
      "386 0.0033120515290647745\n",
      "387 0.0031936028972268105\n",
      "388 0.0030823866836726665\n",
      "389 0.0029718787409365177\n",
      "390 0.0028696598019450903\n",
      "391 0.002767422702163458\n",
      "392 0.002671274356544018\n",
      "393 0.0025789858773350716\n",
      "394 0.002488889265805483\n",
      "395 0.0024049191270023584\n",
      "396 0.00232481537386775\n",
      "397 0.002243933966383338\n",
      "398 0.0021688842680305243\n",
      "399 0.0020963125862181187\n",
      "400 0.002025000751018524\n",
      "401 0.001958113396540284\n",
      "402 0.0018938652938231826\n",
      "403 0.001832172041758895\n",
      "404 0.0017712166300043464\n",
      "405 0.0017151263309642673\n",
      "406 0.0016601583920419216\n",
      "407 0.001606627251021564\n",
      "408 0.0015550400130450726\n",
      "409 0.00150591554120183\n",
      "410 0.0014588363701477647\n",
      "411 0.0014142633881419897\n",
      "412 0.0013689043698832393\n",
      "413 0.0013272003270685673\n",
      "414 0.0012854000087827444\n",
      "415 0.0012456424301490188\n",
      "416 0.0012098023435100913\n",
      "417 0.0011720306938514113\n",
      "418 0.001137051498517394\n",
      "419 0.0011055299546569586\n",
      "420 0.001072519924491644\n",
      "421 0.0010412160772830248\n",
      "422 0.0010089928982779384\n",
      "423 0.0009795641526579857\n",
      "424 0.0009518892038613558\n",
      "425 0.0009240417857654393\n",
      "426 0.0008977363468147814\n",
      "427 0.0008732901769690216\n",
      "428 0.0008490360341966152\n",
      "429 0.0008247762452811003\n",
      "430 0.0008024128037504852\n",
      "431 0.0007784207118675113\n",
      "432 0.0007579615921713412\n",
      "433 0.0007377696456387639\n",
      "434 0.000717389746569097\n",
      "435 0.0006981377955526114\n",
      "436 0.0006799321272410452\n",
      "437 0.0006622598739340901\n",
      "438 0.0006448032218031585\n",
      "439 0.0006284953560680151\n",
      "440 0.0006112924311310053\n",
      "441 0.0005969727062620223\n",
      "442 0.0005811545415781438\n",
      "443 0.0005672704428434372\n",
      "444 0.0005525286542251706\n",
      "445 0.0005399365327320993\n",
      "446 0.0005265538929961622\n",
      "447 0.0005130154895596206\n",
      "448 0.0005008222651667893\n",
      "449 0.0004887291579507291\n",
      "450 0.00047681626165285707\n",
      "451 0.00046629810822196305\n",
      "452 0.0004548384458757937\n",
      "453 0.00044343044282868505\n",
      "454 0.0004337845020927489\n",
      "455 0.00042450818000361323\n",
      "456 0.00041450271965004504\n",
      "457 0.00040479761082679033\n",
      "458 0.0003955208230763674\n",
      "459 0.00038731034146621823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460 0.000378143711714074\n",
      "461 0.0003703972906805575\n",
      "462 0.0003617069451138377\n",
      "463 0.0003543756320141256\n",
      "464 0.00034595231409184635\n",
      "465 0.00033850944600999355\n",
      "466 0.0003316339571028948\n",
      "467 0.0003252212191000581\n",
      "468 0.00031852038227953017\n",
      "469 0.0003111968107987195\n",
      "470 0.0003054046246688813\n",
      "471 0.00029826644458808005\n",
      "472 0.0002926025481428951\n",
      "473 0.0002869728778023273\n",
      "474 0.00028038816526532173\n",
      "475 0.00027491108630783856\n",
      "476 0.00026934704510495067\n",
      "477 0.0002645638887770474\n",
      "478 0.00025881457258947194\n",
      "479 0.0002539958222769201\n",
      "480 0.00024833649513311684\n",
      "481 0.00024413978098891675\n",
      "482 0.00023885704285930842\n",
      "483 0.0002347188419662416\n",
      "484 0.00022988746059127152\n",
      "485 0.00022485006775241345\n",
      "486 0.00022097458713687956\n",
      "487 0.00021710610599257052\n",
      "488 0.0002131229266524315\n",
      "489 0.00020923418924212456\n",
      "490 0.00020525979925878346\n",
      "491 0.00020136861712671816\n",
      "492 0.00019845986389555037\n",
      "493 0.00019467684614937752\n",
      "494 0.00019180012168362737\n",
      "495 0.00018846977036446333\n",
      "496 0.00018524964980315417\n",
      "497 0.00018162558262702078\n",
      "498 0.0001788460649549961\n",
      "499 0.0001755343982949853\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 742.8016967773438\n",
      "1 686.142578125\n",
      "2 636.8507080078125\n",
      "3 593.2828979492188\n",
      "4 554.7335815429688\n",
      "5 520.5545043945312\n",
      "6 489.7171936035156\n",
      "7 461.5309143066406\n",
      "8 435.5517883300781\n",
      "9 411.4443054199219\n",
      "10 388.98382568359375\n",
      "11 367.82623291015625\n",
      "12 347.8412780761719\n",
      "13 329.0830993652344\n",
      "14 311.28863525390625\n",
      "15 294.4093017578125\n",
      "16 278.3799743652344\n",
      "17 263.15179443359375\n",
      "18 248.65359497070312\n",
      "19 234.85372924804688\n",
      "20 221.78561401367188\n",
      "21 209.33975219726562\n",
      "22 197.5129852294922\n",
      "23 186.20469665527344\n",
      "24 175.49124145507812\n",
      "25 165.32415771484375\n",
      "26 155.71221923828125\n",
      "27 146.64251708984375\n",
      "28 138.01995849609375\n",
      "29 129.85888671875\n",
      "30 122.15640258789062\n",
      "31 114.8960952758789\n",
      "32 108.03673553466797\n",
      "33 101.56907653808594\n",
      "34 95.47881317138672\n",
      "35 89.73491668701172\n",
      "36 84.31725311279297\n",
      "37 79.21713256835938\n",
      "38 74.43209838867188\n",
      "39 69.92500305175781\n",
      "40 65.68978881835938\n",
      "41 61.71757888793945\n",
      "42 57.97899627685547\n",
      "43 54.474082946777344\n",
      "44 51.19098663330078\n",
      "45 48.10993576049805\n",
      "46 45.21963882446289\n",
      "47 42.5101432800293\n",
      "48 39.9796142578125\n",
      "49 37.6097412109375\n",
      "50 35.387882232666016\n",
      "51 33.29991149902344\n",
      "52 31.344707489013672\n",
      "53 29.516883850097656\n",
      "54 27.804983139038086\n",
      "55 26.200576782226562\n",
      "56 24.69675636291504\n",
      "57 23.286598205566406\n",
      "58 21.960084915161133\n",
      "59 20.717042922973633\n",
      "60 19.553730010986328\n",
      "61 18.462705612182617\n",
      "62 17.438735961914062\n",
      "63 16.477861404418945\n",
      "64 15.576021194458008\n",
      "65 14.731023788452148\n",
      "66 13.93734359741211\n",
      "67 13.190654754638672\n",
      "68 12.491223335266113\n",
      "69 11.830133438110352\n",
      "70 11.206016540527344\n",
      "71 10.616774559020996\n",
      "72 10.062312126159668\n",
      "73 9.540674209594727\n",
      "74 9.049141883850098\n",
      "75 8.586801528930664\n",
      "76 8.150832176208496\n",
      "77 7.740208625793457\n",
      "78 7.352674961090088\n",
      "79 6.9868035316467285\n",
      "80 6.6418328285217285\n",
      "81 6.315989971160889\n",
      "82 6.007876396179199\n",
      "83 5.716626167297363\n",
      "84 5.441547870635986\n",
      "85 5.180822372436523\n",
      "86 4.93413782119751\n",
      "87 4.700459957122803\n",
      "88 4.478953838348389\n",
      "89 4.269013404846191\n",
      "90 4.069572448730469\n",
      "91 3.880617380142212\n",
      "92 3.7011349201202393\n",
      "93 3.530778408050537\n",
      "94 3.3688437938690186\n",
      "95 3.2152559757232666\n",
      "96 3.0691580772399902\n",
      "97 2.930379629135132\n",
      "98 2.798619031906128\n",
      "99 2.673311233520508\n",
      "100 2.5541110038757324\n",
      "101 2.440681219100952\n",
      "102 2.3327460289001465\n",
      "103 2.2300240993499756\n",
      "104 2.1321702003479004\n",
      "105 2.039024591445923\n",
      "106 1.950297236442566\n",
      "107 1.8656939268112183\n",
      "108 1.7850573062896729\n",
      "109 1.7081396579742432\n",
      "110 1.6348224878311157\n",
      "111 1.5649181604385376\n",
      "112 1.4981696605682373\n",
      "113 1.4345189332962036\n",
      "114 1.3737300634384155\n",
      "115 1.3157174587249756\n",
      "116 1.2603527307510376\n",
      "117 1.2073838710784912\n",
      "118 1.156830072402954\n",
      "119 1.1085484027862549\n",
      "120 1.0624359846115112\n",
      "121 1.018372893333435\n",
      "122 0.9762282967567444\n",
      "123 0.9359404444694519\n",
      "124 0.8974159359931946\n",
      "125 0.8605599999427795\n",
      "126 0.8253118991851807\n",
      "127 0.7916755080223083\n",
      "128 0.7594442963600159\n",
      "129 0.7286235690116882\n",
      "130 0.6991092562675476\n",
      "131 0.6708826422691345\n",
      "132 0.6439021825790405\n",
      "133 0.618026077747345\n",
      "134 0.5932672619819641\n",
      "135 0.5695449709892273\n",
      "136 0.5468380451202393\n",
      "137 0.5251006484031677\n",
      "138 0.5043138265609741\n",
      "139 0.48436349630355835\n",
      "140 0.4652462303638458\n",
      "141 0.4469340443611145\n",
      "142 0.4293903112411499\n",
      "143 0.41256582736968994\n",
      "144 0.3964756727218628\n",
      "145 0.38104942440986633\n",
      "146 0.36624830961227417\n",
      "147 0.35205501317977905\n",
      "148 0.3384312093257904\n",
      "149 0.32536375522613525\n",
      "150 0.31295087933540344\n",
      "151 0.3010762333869934\n",
      "152 0.2896778881549835\n",
      "153 0.2787564694881439\n",
      "154 0.26826661825180054\n",
      "155 0.25819775462150574\n",
      "156 0.24854376912117004\n",
      "157 0.2392716258764267\n",
      "158 0.23036184906959534\n",
      "159 0.22180986404418945\n",
      "160 0.2135881632566452\n",
      "161 0.20569296181201935\n",
      "162 0.198109969496727\n",
      "163 0.1908225119113922\n",
      "164 0.18382935225963593\n",
      "165 0.17711098492145538\n",
      "166 0.17065425217151642\n",
      "167 0.1644510179758072\n",
      "168 0.15849089622497559\n",
      "169 0.15276069939136505\n",
      "170 0.1472427397966385\n",
      "171 0.1419396549463272\n",
      "172 0.13683754205703735\n",
      "173 0.1319303810596466\n",
      "174 0.12721076607704163\n",
      "175 0.12267354875802994\n",
      "176 0.1183059960603714\n",
      "177 0.11410420387983322\n",
      "178 0.11006126552820206\n",
      "179 0.10617292672395706\n",
      "180 0.10243330895900726\n",
      "181 0.09882897138595581\n",
      "182 0.09535964578390121\n",
      "183 0.09201890230178833\n",
      "184 0.08880159258842468\n",
      "185 0.0857023373246193\n",
      "186 0.08271779865026474\n",
      "187 0.0798451378941536\n",
      "188 0.07707832008600235\n",
      "189 0.07441358268260956\n",
      "190 0.07184207439422607\n",
      "191 0.06936827301979065\n",
      "192 0.0669851079583168\n",
      "193 0.06468647718429565\n",
      "194 0.06247207894921303\n",
      "195 0.060332246124744415\n",
      "196 0.0582696795463562\n",
      "197 0.05628301948308945\n",
      "198 0.05436687171459198\n",
      "199 0.052519429475069046\n",
      "200 0.050739213824272156\n",
      "201 0.04902496188879013\n",
      "202 0.047370489686727524\n",
      "203 0.045776382088661194\n",
      "204 0.044236645102500916\n",
      "205 0.04275169223546982\n",
      "206 0.04132010415196419\n",
      "207 0.0399390384554863\n",
      "208 0.03860877454280853\n",
      "209 0.03732483461499214\n",
      "210 0.036084454506635666\n",
      "211 0.03488754853606224\n",
      "212 0.033732883632183075\n",
      "213 0.03261924162507057\n",
      "214 0.0315437950193882\n",
      "215 0.03050556592643261\n",
      "216 0.02950265444815159\n",
      "217 0.02853504940867424\n",
      "218 0.027601327747106552\n",
      "219 0.026699211448431015\n",
      "220 0.02582775056362152\n",
      "221 0.02498668059706688\n",
      "222 0.02417539805173874\n",
      "223 0.023391423746943474\n",
      "224 0.022634075954556465\n",
      "225 0.021902933716773987\n",
      "226 0.021196309477090836\n",
      "227 0.020513227209448814\n",
      "228 0.01985372044146061\n",
      "229 0.01921619474887848\n",
      "230 0.01860092766582966\n",
      "231 0.018005916848778725\n",
      "232 0.017430759966373444\n",
      "233 0.016876159235835075\n",
      "234 0.016340000554919243\n",
      "235 0.01582076959311962\n",
      "236 0.01531838346272707\n",
      "237 0.014833134599030018\n",
      "238 0.014364294707775116\n",
      "239 0.013910976238548756\n",
      "240 0.013472345657646656\n",
      "241 0.013048341497778893\n",
      "242 0.012638542801141739\n",
      "243 0.01224235538393259\n",
      "244 0.011858562007546425\n",
      "245 0.011487548239529133\n",
      "246 0.011129050515592098\n",
      "247 0.010781990364193916\n",
      "248 0.01044608186930418\n",
      "249 0.0101212989538908\n",
      "250 0.009807257913053036\n",
      "251 0.009503382258117199\n",
      "252 0.009209392592310905\n",
      "253 0.008924653753638268\n",
      "254 0.008649265393614769\n",
      "255 0.008382834494113922\n",
      "256 0.00812492985278368\n",
      "257 0.007875299081206322\n",
      "258 0.007633597124367952\n",
      "259 0.007399626541882753\n",
      "260 0.0071732522919774055\n",
      "261 0.006954253651201725\n",
      "262 0.0067421491257846355\n",
      "263 0.006536685395985842\n",
      "264 0.006337953731417656\n",
      "265 0.006145298480987549\n",
      "266 0.005958799738436937\n",
      "267 0.005778222810477018\n",
      "268 0.005603338126093149\n",
      "269 0.0054340423084795475\n",
      "270 0.005270154215395451\n",
      "271 0.005111193750053644\n",
      "272 0.0049572717398405075\n",
      "273 0.004808158613741398\n",
      "274 0.004663870669901371\n",
      "275 0.004523863084614277\n",
      "276 0.004388291854411364\n",
      "277 0.004257050342857838\n",
      "278 0.004129950422793627\n",
      "279 0.004006639122962952\n",
      "280 0.0038871378637850285\n",
      "281 0.003771375399082899\n",
      "282 0.00365920620970428\n",
      "283 0.003550468711182475\n",
      "284 0.0034450755920261145\n",
      "285 0.0033430277835577726\n",
      "286 0.003244166960939765\n",
      "287 0.0031481562182307243\n",
      "288 0.003055143402889371\n",
      "289 0.002964993240311742\n",
      "290 0.002877650083974004\n",
      "291 0.0027929027564823627\n",
      "292 0.00271079340018332\n",
      "293 0.0026312433183193207\n",
      "294 0.002554098144173622\n",
      "295 0.00247924099676311\n",
      "296 0.0024066236801445484\n",
      "297 0.002336220582947135\n",
      "298 0.002267994452267885\n",
      "299 0.002201745519414544\n",
      "300 0.0021376872900873423\n",
      "301 0.002075646771118045\n",
      "302 0.002015277510508895\n",
      "303 0.001956744585186243\n",
      "304 0.0018999868771061301\n",
      "305 0.001844896818511188\n",
      "306 0.0017914611380547285\n",
      "307 0.0017395926406607032\n",
      "308 0.001689335098490119\n",
      "309 0.0016405065543949604\n",
      "310 0.0015931854723021388\n",
      "311 0.001547238789498806\n",
      "312 0.001502635539509356\n",
      "313 0.0014593680389225483\n",
      "314 0.0014174473471939564\n",
      "315 0.0013767890632152557\n",
      "316 0.0013372651301324368\n",
      "317 0.0012989107053726912\n",
      "318 0.001261712983250618\n",
      "319 0.0012256006011739373\n",
      "320 0.0011905409628525376\n",
      "321 0.00115652394015342\n",
      "322 0.0011235354468226433\n",
      "323 0.0010915006278082728\n",
      "324 0.0010603968985378742\n",
      "325 0.001030178740620613\n",
      "326 0.0010008933022618294\n",
      "327 0.0009724677656777203\n",
      "328 0.000944864412304014\n",
      "329 0.0009180543711408973\n",
      "330 0.0008920278050936759\n",
      "331 0.0008667727815918624\n",
      "332 0.0008422316168434918\n",
      "333 0.0008184122270904481\n",
      "334 0.0007953387103043497\n",
      "335 0.0007729767239652574\n",
      "336 0.0007512455340474844\n",
      "337 0.0007301421137526631\n",
      "338 0.0007096594199538231\n",
      "339 0.000689786858856678\n",
      "340 0.0006704626721329987\n",
      "341 0.0006517061265185475\n",
      "342 0.0006334900390356779\n",
      "343 0.000615784665569663\n",
      "344 0.0005985859897918999\n",
      "345 0.0005818785284645855\n",
      "346 0.00056567502906546\n",
      "347 0.0005499503458850086\n",
      "348 0.0005346314865164459\n",
      "349 0.0005197644932195544\n",
      "350 0.0005053202621638775\n",
      "351 0.0004912890144623816\n",
      "352 0.0004776548594236374\n",
      "353 0.0004644215223379433\n",
      "354 0.00045154988765716553\n",
      "355 0.00043905508937314153\n",
      "356 0.0004269044438842684\n",
      "357 0.0004150934692006558\n",
      "358 0.00040362696745432913\n",
      "359 0.00039249431574717164\n",
      "360 0.00038165514706633985\n",
      "361 0.0003711268655024469\n",
      "362 0.0003609053965192288\n",
      "363 0.000350966933183372\n",
      "364 0.0003413071681279689\n",
      "365 0.00033191777765750885\n",
      "366 0.0003227962297387421\n",
      "367 0.00031394240795634687\n",
      "368 0.00030532164964824915\n",
      "369 0.00029694897239096463\n",
      "370 0.00028880793252028525\n",
      "371 0.00028089972329325974\n",
      "372 0.00027320621302351356\n",
      "373 0.0002657255681697279\n",
      "374 0.00025844728224910796\n",
      "375 0.00025140619254671037\n",
      "376 0.00024454211234115064\n",
      "377 0.00023787714599166065\n",
      "378 0.00023140470148064196\n",
      "379 0.00022510203416459262\n",
      "380 0.00021897927217651159\n",
      "381 0.00021301342349033803\n",
      "382 0.0002072150236926973\n",
      "383 0.0002015889622271061\n",
      "384 0.0001961064408533275\n",
      "385 0.00019078774494118989\n",
      "386 0.0001856127637438476\n",
      "387 0.00018057865963783115\n",
      "388 0.000175682915141806\n",
      "389 0.00017092644702643156\n",
      "390 0.00016630043683107942\n",
      "391 0.00016179851081687957\n",
      "392 0.00015742242976557463\n",
      "393 0.00015317041834350675\n",
      "394 0.00014903141709510237\n",
      "395 0.00014501091209240258\n",
      "396 0.0001410976256011054\n",
      "397 0.00013728694466408342\n",
      "398 0.0001335882698185742\n",
      "399 0.0001299873983953148\n",
      "400 0.00012649130076169968\n",
      "401 0.00012308426084928215\n",
      "402 0.0001197706296807155\n",
      "403 0.00011655564594548196\n",
      "404 0.00011342274228809401\n",
      "405 0.00011037600779673085\n",
      "406 0.00010741114965640008\n",
      "407 0.00010453213326400146\n",
      "408 0.00010172754264203832\n",
      "409 9.899900760501623e-05\n",
      "410 9.634133311919868e-05\n",
      "411 9.376298839924857e-05\n",
      "412 9.125154610956088e-05\n",
      "413 8.881070971256122e-05\n",
      "414 8.643455657875165e-05\n",
      "415 8.412245369981974e-05\n",
      "416 8.187801722669974e-05\n",
      "417 7.968796853674576e-05\n",
      "418 7.755963451927528e-05\n",
      "419 7.548861321993172e-05\n",
      "420 7.347446080530062e-05\n",
      "421 7.151602767407894e-05\n",
      "422 6.960770406294614e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 6.775365181965753e-05\n",
      "424 6.594568549189717e-05\n",
      "425 6.419258716050535e-05\n",
      "426 6.248038698686287e-05\n",
      "427 6.082204345148057e-05\n",
      "428 5.9202029660809785e-05\n",
      "429 5.7630848459666595e-05\n",
      "430 5.6098793720593676e-05\n",
      "431 5.46075934835244e-05\n",
      "432 5.316117312759161e-05\n",
      "433 5.174609032110311e-05\n",
      "434 5.0373935664538294e-05\n",
      "435 4.903993249172345e-05\n",
      "436 4.7737135901115835e-05\n",
      "437 4.647467358154245e-05\n",
      "438 4.5241689804242924e-05\n",
      "439 4.404642095323652e-05\n",
      "440 4.2877749365288764e-05\n",
      "441 4.174300192971714e-05\n",
      "442 4.063723827130161e-05\n",
      "443 3.956340151489712e-05\n",
      "444 3.851810470223427e-05\n",
      "445 3.7500787584576756e-05\n",
      "446 3.65080704796128e-05\n",
      "447 3.5547658626455814e-05\n",
      "448 3.460848529357463e-05\n",
      "449 3.369620753801428e-05\n",
      "450 3.2808748073875904e-05\n",
      "451 3.194475721102208e-05\n",
      "452 3.110207399004139e-05\n",
      "453 3.0281004001153633e-05\n",
      "454 2.9485132472473197e-05\n",
      "455 2.8706846933346242e-05\n",
      "456 2.7952033633482642e-05\n",
      "457 2.721598866628483e-05\n",
      "458 2.649847920110915e-05\n",
      "459 2.5804109100135975e-05\n",
      "460 2.5124618332483806e-05\n",
      "461 2.446494545438327e-05\n",
      "462 2.3822387447580695e-05\n",
      "463 2.3197933842311613e-05\n",
      "464 2.258761196571868e-05\n",
      "465 2.199397567892447e-05\n",
      "466 2.1417170501081273e-05\n",
      "467 2.0856334231211804e-05\n",
      "468 2.0308767489041202e-05\n",
      "469 1.977581632672809e-05\n",
      "470 1.9256334780948237e-05\n",
      "471 1.8753096810542047e-05\n",
      "472 1.826028346840758e-05\n",
      "473 1.7783149814931676e-05\n",
      "474 1.7318645404884592e-05\n",
      "475 1.6865389625309035e-05\n",
      "476 1.6424746718257666e-05\n",
      "477 1.5995867215679027e-05\n",
      "478 1.5575953511870466e-05\n",
      "479 1.5169533071457408e-05\n",
      "480 1.4772128452023026e-05\n",
      "481 1.438714662072016e-05\n",
      "482 1.401115059707081e-05\n",
      "483 1.3644599675899372e-05\n",
      "484 1.3287964975461364e-05\n",
      "485 1.2942128705617506e-05\n",
      "486 1.2603411960299127e-05\n",
      "487 1.2275959306862205e-05\n",
      "488 1.1955125955864787e-05\n",
      "489 1.1643683137663174e-05\n",
      "490 1.1338952390360646e-05\n",
      "491 1.104454440792324e-05\n",
      "492 1.0755477887869347e-05\n",
      "493 1.047546902555041e-05\n",
      "494 1.0203459169133566e-05\n",
      "495 9.937532922776882e-06\n",
      "496 9.679587492428254e-06\n",
      "497 9.42751194088487e-06\n",
      "498 9.181761015497614e-06\n",
      "499 8.942743988882285e-06\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "\n",
    "# Code in file nn/two_layer_net_nn.py\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# After constructing the model we use the .to() method to move it to the\n",
    "# desired device.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        ).to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "# reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "# than the mean; this is for consistency with the examples above where we\n",
    "# manually compute the loss, but in practice it is more common to use mean\n",
    "# squared error as a loss by setting reduction='elementwise_mean'.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "  # override the __call__ operator so you can call them like functions. When\n",
    "  # doing so you pass a Tensor of input data to the Module and it produces\n",
    "  # a Tensor of output data.\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "  # values of y, and the loss function returns a Tensor containing the loss.\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  print(t, loss.item())\n",
    "  \n",
    "  # Zero the gradients before running the backward pass.\n",
    "  model.zero_grad()\n",
    "\n",
    "  # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "  # parameters of the model. Internally, the parameters of each Module are stored\n",
    "  # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "  # all learnable parameters in the model.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "  # we can access its data and gradients like we did before.\n",
    "  with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "      param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-57485dc19e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create placeholders for the input and target data; these will be filled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# with real data when we execute the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/tf_two_layer_net.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "  # Run the graph once to initialize the Variables w1 and w2.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # Create numpy arrays holding the actual data for the inputs x and targets y\n",
    "  x_value = np.random.randn(N, D_in)\n",
    "  y_value = np.random.randn(N, D_out)\n",
    "  for _ in range(500):\n",
    "    # Execute the graph many times. Each time it executes we want to bind\n",
    "    # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "    # Each time we execute the graph we want to compute the values for loss,\n",
    "    # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "    # arrays.\n",
    "    loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                feed_dict={x: x_value, y: y_value})\n",
    "    print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Small Research/Thinking Activity - Cost-Sensitive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
