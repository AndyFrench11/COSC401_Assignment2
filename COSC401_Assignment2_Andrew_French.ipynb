{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC401 Assignment 2\n",
    "## Andrew French \n",
    "### ID: 11147452"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is all of the libraries and modules needed for the entire program being imported. \n",
    "Make sure to run this step before running any of the feature code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is all of the variables set that are global to the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19680801)\n",
    "plt.rcParams['figure.figsize'] = (10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Gradient-based Learning with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 1000\n",
    "RANDOM_STD_DEV = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseFunction():\n",
    "    return random.gauss(1, RANDOM_STD_DEV)\n",
    "\n",
    "def ran_int():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "def inputFunc1(x1, x2, x4):\n",
    "    return x1 + x2 - x4 + noiseFunction()\n",
    "\n",
    "def inputFunc2(x2, x3):\n",
    "    return x2 * 3 + x3 + noiseFunction()\n",
    "\n",
    "def inputFunc3(x2, x4):\n",
    "    return x2 + x4 + 2 + noiseFunction()\n",
    "\n",
    "def inputFunc4(x1, x3):\n",
    "    return (x1 - x3) * 2 + noiseFunction()\n",
    "\n",
    "def inputFunc5(x1, x4):\n",
    "    return (x4 + x1) - 6 + noiseFunction()\n",
    "\n",
    "def modelFunc(x, w, b):\n",
    "    \"\"\"Performs matrix multiplication...\"\"\"\n",
    "    return x @ w.t() + b\n",
    "\n",
    "def mse(t1, t2):\n",
    "    \"\"\"Mean Squared Error loss function\"\"\"\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_list = []\n",
    "target_data_list = []\n",
    "\n",
    "for i in range(DATA_SIZE):\n",
    "    x1, x2, x3, x4 = ran_int(), ran_int(), ran_int(), ran_int()\n",
    "    input_data_list.append([x1, x2, x3, x4])\n",
    "    target_data_list.append([inputFunc1(x1, x2, x4), inputFunc2(x2, x3), inputFunc3(x2, x4), inputFunc4(x1, x3), \n",
    "                      inputFunc5(x1, x4)])\n",
    "    \n",
    "input_data = np.array(input_data_list, dtype='float32')\n",
    "target_data = np.array(target_data_list, dtype='float32')\n",
    "\n",
    "#Convert to tensors\n",
    "inputs = torch.from_numpy(input_data)\n",
    "targets = torch.from_numpy(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss 24602.76171875\n",
      "Iteration: 10000, Loss 0.7094482779502869\n",
      "Iteration: 20000, Loss 0.6653133034706116\n",
      "Iteration: 30000, Loss 0.6239269375801086\n",
      "Iteration: 40000, Loss 0.5851123332977295\n",
      "Iteration: 50000, Loss 0.5487135648727417\n",
      "Iteration: 60000, Loss 0.5145819783210754\n",
      "Iteration: 70000, Loss 0.4825710952281952\n",
      "Iteration: 80000, Loss 0.4525541067123413\n",
      "Iteration: 90000, Loss 0.42440658807754517\n",
      "Iteration: 100000, Loss 0.3980015218257904\n",
      "Iteration: 110000, Loss 0.3732510209083557\n",
      "Iteration: 120000, Loss 0.3500339090824127\n",
      "Iteration: 130000, Loss 0.3282581865787506\n",
      "Iteration: 140000, Loss 0.3078409731388092\n",
      "Iteration: 150000, Loss 0.2886984348297119\n",
      "Iteration: 160000, Loss 0.2707436680793762\n",
      "Iteration: 170000, Loss 0.2539035677909851\n",
      "Iteration: 180000, Loss 0.23811420798301697\n",
      "Iteration: 190000, Loss 0.2233061045408249\n",
      "Iteration: 200000, Loss 0.2094200700521469\n",
      "Iteration: 210000, Loss 0.19639381766319275\n",
      "Iteration: 220000, Loss 0.18419933319091797\n",
      "Iteration: 230000, Loss 0.17273564636707306\n",
      "Iteration: 240000, Loss 0.1620052605867386\n",
      "Iteration: 250000, Loss 0.1519431471824646\n",
      "Iteration: 260000, Loss 0.14249175786972046\n",
      "Iteration: 270000, Loss 0.13364888727664948\n",
      "Iteration: 280000, Loss 0.1253279596567154\n",
      "Iteration: 290000, Loss 0.11754334717988968\n",
      "Iteration: 300000, Loss 0.11024335026741028\n",
      "Iteration: 310000, Loss 0.1033763661980629\n",
      "Iteration: 320000, Loss 0.09696520864963531\n",
      "Iteration: 330000, Loss 0.09094289690256119\n",
      "Iteration: 340000, Loss 0.08527815341949463\n",
      "Iteration: 350000, Loss 0.07998241484165192\n",
      "Iteration: 360000, Loss 0.07500825077295303\n",
      "Iteration: 370000, Loss 0.07034646719694138\n",
      "Iteration: 380000, Loss 0.06597918272018433\n",
      "Iteration: 390000, Loss 0.06189657002687454\n",
      "Iteration: 400000, Loss 0.058042529970407486\n",
      "Iteration: 410000, Loss 0.05443236604332924\n",
      "Iteration: 420000, Loss 0.05105731263756752\n",
      "Iteration: 430000, Loss 0.047890663146972656\n",
      "Iteration: 440000, Loss 0.04492214322090149\n",
      "Iteration: 450000, Loss 0.04213689640164375\n"
     ]
    }
   ],
   "source": [
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w = torch.randn(5, 4, requires_grad=True)\n",
    "b = torch.randn(5, requires_grad=True)\n",
    "\n",
    "preds = modelFunc(inputs, w, b)\n",
    "loss = mse(preds, targets)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "count = 0\n",
    "while count < 1000000 and loss.item() > 0.04:\n",
    "    preds = modelFunc(inputs, w, b)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"Iteration: {count}, Loss {loss.item()}\")\n",
    "    count += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-57485dc19e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create placeholders for the input and target data; these will be filled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# with real data when we execute the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Small Research/Thinking Activity - Cost-Sensitive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are given a k-by-k cost matrix C for a classification task which\n",
    "has k classes. The element Ci,j is the cost of classifying an instance of class j\n",
    "as class i. You want to train a classifier that minimises the expected cost of\n",
    "predictions. Do the following:\n",
    "1. Write a pseudocode or Python code for a loss function that, once optimised\n",
    "over all examples, achieves minimum expected cost of prediction.\n",
    "2. Assuming that a learning algorithm which minimises the classification\n",
    "error is given to you (and that you cannot supply your own loss function\n",
    "to the algorithm), think and write about a different way of achieving\n",
    "minimum expected cost of prediction.\n",
    "Additional notes or requirements:\n",
    "• Consider cases with k ≥ 2 and briefly discuss if the two methods are\n",
    "scalable.\n",
    "• You do not have to use tensors in this questions.\n",
    "• If you choose to write in Python, you do not have to run your program\n",
    "on any particular input. You can also use “pseudo-Python” if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2 - Achieving minimym expected cost of prediction\n",
    "Idea: if you have a classifer with lots of data and one without, reduce the size of the data to match the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
